{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "400594ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a0f0db",
   "metadata": {},
   "source": [
    "### Reading JSON FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b3f8a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function opens json file and returns it in data\n",
    "def open_js_file(name):\n",
    "    #t1 = time.time()\n",
    "    json_ = open(name)\n",
    "    data = json.load(json_)\n",
    "    #t2 = time.time()\n",
    "    #print('Needed {} sec = {} min'.format(t2-t1, (t2-t1)/60))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06cf08",
   "metadata": {},
   "source": [
    "### Getting accelerations and metainformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a47ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function gets json file (data) as input and returns acceleration data, vehicle-ID, crossing speed and labels\n",
    "def get_acceleration_fid_v_labels(data, va=False, ha=False): \n",
    "    # data = js FILE\n",
    "    # va=False, ha=False-> roi not separated\n",
    "    # va=True -> acceleration data of roi_va\n",
    "    # ha=True -> acceleration data of roi_ha\n",
    "    \n",
    "    # dataframe for acceleration measurement series\n",
    "    shape = len(data['0th crossing'][2]['accz'])\n",
    "    df = np.zeros((len(data), shape, 3))\n",
    "    \n",
    "    # label vorderachse, label hinterachse, fahrzeug-id, geschwindigkeit\n",
    "    l_va, l_ha, fid, v =[], [], [], []\n",
    "    \n",
    "    # idx1 = index of label va, idx2 = index pf label ha\n",
    "    # index of label va/ label ha depends on chosen dataset (9 or 10)\n",
    "    if va == True: \n",
    "        idx1 = 9\n",
    "    elif ha == True:\n",
    "        idx2 = 9\n",
    "    elif va == False and ha == False: \n",
    "        idx1 = 9\n",
    "        idx2 = 10\n",
    "        \n",
    "    # save acceleration data of every crossing (len(data)=number of crossings)    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        # acceleration data in x, y, z direction\n",
    "        df[i,:,0] = data[str(i)+'th crossing'][0]['accx'] \n",
    "        df[i,:,1] = data[str(i)+'th crossing'][1]['accy'] \n",
    "        df[i,:,2] = data[str(i)+'th crossing'][2]['accz'] \n",
    "        \n",
    "        # vehicle id\n",
    "        fid = np.append(fid, data[str(i)+'th crossing'][4]['fid'])\n",
    "        # speed of the crossing\n",
    "        v = np.append(v, data[str(i)+'th crossing'][7]['geschwindigkeit']) \n",
    "        \n",
    "        # differentiate between datasets \n",
    "        # separated va -> label va is needed only \n",
    "        # separated ha -> label ha is needed only\n",
    "        # not separated -> label va and label ha are needed both\n",
    "        if va == True: \n",
    "            l_va = np.append(l_va, data[str(i)+'th crossing'][idx1]['label_va'])\n",
    "        elif ha == True:\n",
    "            l_ha = np.append(l_ha, data[str(i)+'th crossing'][idx2]['label_ha'])\n",
    "        elif va == False and ha == False: \n",
    "            l_va = np.append(l_va, data[str(i)+'th crossing'][idx1]['label_va'])\n",
    "            l_ha = np.append(l_ha, data[str(i)+'th crossing'][idx2]['label_ha'])\n",
    "    \n",
    "    # return statements (depend ondataset)\n",
    "    if va == False and ha == False: \n",
    "        return df, fid, v, l_va, l_ha\n",
    "    elif va == True:\n",
    "        return df, fid, v, l_va\n",
    "    elif ha == True:\n",
    "        return df, fid, v, l_ha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89476b",
   "metadata": {},
   "source": [
    "#### If va and ha not separated, there are 4 given classes.\n",
    "Build the class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "199d2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 0 = defect, label 1 = intact\n",
    "def labels_roi(lva, lha):\n",
    "    l_roi=[]\n",
    "    #class0: va defect & ha defect\n",
    "    #class0: va defect & ha intact\n",
    "    #class0: va intact & ha defect\n",
    "    #class0: va intact & ha intact\n",
    "    for i in range(len(lva)):\n",
    "        if lva[i]==0.0 and lha[i]==0.0:\n",
    "            l_roi = np.append(l_roi, 0)\n",
    "        elif lva[i]==0.0 and lha[i]==1.0:\n",
    "            l_roi = np.append(l_roi, 1)\n",
    "        elif lva[i]==1.0 and lha[i]==0.0:\n",
    "            l_roi = np.append(l_roi, 2)\n",
    "        elif lva[i]==1.0 and lha[i]==1.0:\n",
    "            l_roi = np.append(l_roi, 3)\n",
    "    return l_roi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ed7eb",
   "metadata": {},
   "source": [
    "#### Have a look at the vihicle ids (fid):\n",
    "Save the fids (delete the double ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8e50dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fids(fid):\n",
    "    f = []\n",
    "    #idx = []\n",
    "    \n",
    "    # go through all fids\n",
    "    for i in range(1,len(fid)):\n",
    "        \n",
    "        # save the current (i-1) fid, if the next one is a new one\n",
    "        if fid[i-1] != fid[i]:\n",
    "            #idx = np.append(idx, i)\n",
    "            f = np.append(f, fid[i-1])\n",
    "            \n",
    "    # delete the fids which are doubled\n",
    "    f = np.unique(f)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6a6d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the distribution of the labels\n",
    "def label_distr(y, va_ha_sep=False):\n",
    "    count0, count1, count2, count3 = 0, 0, 0, 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i]==0:\n",
    "            count0 += 1\n",
    "        elif y[i]==1:\n",
    "            count1 += 1\n",
    "        elif y[i]==2:\n",
    "            count2 += 1\n",
    "        elif y[i]==3:\n",
    "            count3 += 1\n",
    "            \n",
    "    #print('Distribution labels:',round(count0/len(y),4), round(count1/len(y),4), round(count2/len(y),4), round(count3/len(y),4))  \n",
    "    if va_ha_sep==False:\n",
    "        return np.array([count0/len(y), count1/len(y), count2/len(y), count3/len(y)])\n",
    "    elif va_ha_sep==True:\n",
    "        return np.array([count0/len(y), count1/len(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "465f0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the distribution of the testset and the trainset with the original distribution of the labels in whole dataset\n",
    "#it is allowed to have a deviation of 'percent'%\n",
    "def compare_distr(distr_test, distr_train, distr_roi, percent=10):\n",
    "    \n",
    "    # does the testset has a deviation less than 'percent'% from the original distribution?\n",
    "    d_test = (distr_test < (1+percent/100)*distr_roi).all() and (distr_test > (1-percent/100)*distr_roi).all()\n",
    "    \n",
    "    # does the trainset has a deviation less than 'percent'% from the original distribution?\n",
    "    d_train = (distr_train < (1+percent/100)*distr_roi).all() and (distr_train > (1-percent/100)*distr_roi).all()\n",
    "    \n",
    "    # if test- and trainset both have a deviation less than 'percent'%, set return statement True\n",
    "    same_distr = False\n",
    "    if d_test==True and d_train==True:\n",
    "        same_distr = True    \n",
    "    return same_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0af327bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_test_split(fid, df, l_roi, va_ha_sep=False): \n",
    "    # call get_fids -> f contains the fid's\n",
    "    f = get_fids(fid)\n",
    "    # 1. condition to exit the loop (testdata should not contain more than 21 % of the whole data)\n",
    "    testdata_21per=False\n",
    "    # 2. condition to exit the loop (test- and traindata should have approximatly the same label distribution as the whole dataset)\n",
    "    same_distr = False\n",
    "    # while test_data contains less than 21 % and the distribution is not closely the same do ...\n",
    "    while testdata_21per==False and same_distr==False:\n",
    "        # number of fid's in dataset\n",
    "        n = fid.shape[0]  \n",
    "        # list to save the indices of the testdata\n",
    "        test_idx = []\n",
    "        # list which will contain test data\n",
    "        x_test = []\n",
    "        \n",
    "         # condition that testdata should contain at least 20 percent of whole data\n",
    "        testdata_20per = False\n",
    "        # do the following loop as long as testdata contain less than 20 percent of whole data\n",
    "        while testdata_20per==False:\n",
    "            # random choice of a fid\n",
    "            random_fid = np.random.choice(f)\n",
    "            # Every fid should be chosen one time. Save the indices where random_fid==f, such that these cant be deleted later \n",
    "            r_idx = np.where(random_fid==f)\n",
    "            r_idx = np.array(r_idx)\n",
    "            # reshape it and save it as integer\n",
    "            r_idx = (np.reshape(r_idx, (r_idx.shape[1],))).astype(int)\n",
    "            # delete randomly chosen fid such that it can't be chosen in next loop round\n",
    "            f = np.delete(f, r_idx)\n",
    "            \n",
    "            # x = indices of crossings with the randomly chosen fid of this loop round \n",
    "            x = np.where(random_fid==fid)\n",
    "            x = np.array(x)\n",
    "            # reshape x and save it as int\n",
    "            x = (np.reshape(x, (x.shape[1],))).astype(int)\n",
    "            # save the indices of the crossings which are in testdataset now\n",
    "            test_idx = (np.append(test_idx, x)).astype(int)\n",
    "            \n",
    "            # testset\n",
    "            x_test = np.append(x_test, df[x,:,:])\n",
    "            x_test = np.reshape(x_test, (-1, df.shape[1], df.shape[2]))\n",
    "            \n",
    "            # condition that testdata should contain at least 20% of whole data\n",
    "            if x_test.shape[0] >= 0.2*n: testdata_20per = True\n",
    "        \n",
    "        # crossings = indices of all crossings (is needed to built traindataset in next step)\n",
    "        crossings = np.arange(n)\n",
    "        # to separate test from train, overwrite the indices of test with -1\n",
    "        crossings[test_idx] = -1\n",
    "        # train_idx will contain indices of train crossings\n",
    "        train_idx = []\n",
    "        # fill train_dix with the values of crossings where crossings isn't -1 (crossings[i]!=-1) and save them as integer\n",
    "        for i in range(n):\n",
    "            if crossings[i]!=-1:\n",
    "                train_idx = (np.append(train_idx, crossings[i])).astype(int)\n",
    "        \n",
    "        # save the corresponding labels into y_train and y_test\n",
    "        y_test = l_roi[test_idx]\n",
    "        y_train = l_roi[train_idx]\n",
    "        \n",
    "        # calculate the distribution of the labels in test and traindata\n",
    "        distr_test = label_distr(y_test, va_ha_sep)\n",
    "        distr_train = label_distr(y_train, va_ha_sep)\n",
    "        distr_roi = label_distr(l_roi, va_ha_sep)\n",
    "        \n",
    "        # compare the distributions of the labels in test and traindata with the distribution of the labels in whole data\n",
    "        same_distr = compare_distr(distr_test, distr_train, distr_roi)\n",
    "        # condition to stop the loop, if testdataset gets bigger than 21% of the whole dataset\n",
    "        if x_test.shape[0]<0.21*n: testdata_21per = True\n",
    "    \n",
    "    # save the traindataset        \n",
    "    x_train = df[train_idx,:,:]\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce3cdcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_cross_validation(fid, df, l_roi, before, va_ha_sep=False): \n",
    "    # call get_fids -> f contains the fid's\n",
    "    f = get_fids(fid)\n",
    "    # 1. condition to exit the loop (testdata should not contain more than 21 % of the whole data)\n",
    "    testdata_21per=False\n",
    "    test_crossvalidation  = False\n",
    "    # 2. condition to exit the loop (test- and traindata should have approximatly the same label distribution as the whole dataset)\n",
    "    same_distr = False\n",
    "    # while test_data contains less than 21 % and the distribution is not closely the same do ...\n",
    "    while testdata_21per==False and same_distr==False:\n",
    "        # number of fid's in dataset\n",
    "        n = fid.shape[0]  \n",
    "        # list to save the indices of the testdata\n",
    "        test_idx = []\n",
    "        # list which will contain test data\n",
    "        x_test = []\n",
    "        test = []\n",
    "        # condition that testdata should contain at least 20 percent of whole data\n",
    "        testdata_20per = False\n",
    "        # do the following loop as long as testdata contain less than 20 percent of whole data\n",
    "        while testdata_20per==False:\n",
    "            # random choice of a fid\n",
    "            random_fid = np.random.choice(f)\n",
    "            test.append(int(random_fid))\n",
    "            # Every fid should be chosen one time. Save the indices where random_fid==f, such that these cant be deleted later \n",
    "            r_idx = np.where(random_fid==f)\n",
    "            r_idx = np.array(r_idx)\n",
    "            # reshape it and save it as integer\n",
    "            r_idx = (np.reshape(r_idx, (r_idx.shape[1],))).astype(int)\n",
    "            # delete randomly chosen fid such that it can't be chosen in next loop round\n",
    "            f = np.delete(f, r_idx)           \n",
    "            # x = indices of crossings with the randomly chosen fid of this loop round \n",
    "            x = np.where(random_fid==fid)\n",
    "            x = np.array(x)\n",
    "            # reshape x and save it as int\n",
    "            x = (np.reshape(x, (x.shape[1],))).astype(int)\n",
    "            # save the indices of the crossings which are in testdataset now\n",
    "            if x not in test_idx:\n",
    "                test_idx = (np.append(test_idx, x)).astype(int)\n",
    "                # testset\n",
    "                x_test = np.append(x_test, df[x,:,:])\n",
    "                x_test = np.reshape(x_test, (-1, df.shape[1], df.shape[2]))\n",
    "            # condition that testdata should contain at least 20% of whole data\n",
    "            if x_test.shape[0] >= 0.2*n: testdata_20per = True\n",
    "        for i in range(len(test)):\n",
    "            swap = i + np.argmin(test[i:])\n",
    "            (test[i], test[swap]) = (test[swap], test[i])\n",
    "        # crossings = indices of all crossings (is needed to built traindataset in next step)\n",
    "        crossings = np.arange(n)\n",
    "        # to separate test from train, overwrite the indices of test with -1\n",
    "        crossings[test_idx] = -1\n",
    "        # train_idx will contain indices of train crossings\n",
    "        train_idx = []\n",
    "        # fill train_dix with the values of crossings where crossings isn't -1 (crossings[i]!=-1) and save them as integer\n",
    "        for i in range(n):\n",
    "            if crossings[i]!=-1:\n",
    "                train_idx = (np.append(train_idx, crossings[i])).astype(int)\n",
    "        \n",
    "        # save the corresponding labels into y_train and y_test\n",
    "        y_test = l_roi[test_idx]\n",
    "        y_train = l_roi[train_idx]\n",
    "        \n",
    "        # calculate the distribution of the labels in test and traindata\n",
    "        distr_test = label_distr(y_test, va_ha_sep)\n",
    "        distr_train = label_distr(y_train, va_ha_sep)\n",
    "        distr_roi = label_distr(l_roi, va_ha_sep)\n",
    "        \n",
    "        # compare the distributions of the labels in test and traindata with the distribution of the labels in whole data\n",
    "        same_distr = compare_distr(distr_test, distr_train, distr_roi)\n",
    "        # condition to stop the loop, if testdataset gets bigger than 21% of the whole dataset\n",
    "        if x_test.shape[0]<0.21*n: testdata_21per = True\n",
    "        if test not in before: test_crossvalidation = True\n",
    "    before.append(test)\n",
    "    # save the traindataset        \n",
    "    x_train = df[train_idx,:,:]\n",
    "    return x_train, y_train, x_test, y_test, before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03c8008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bring_in_right_shape_self(x_train, y_train, x_test, y_test, num_classes=4, num_features=512*3):\n",
    "    #from tensorflow import to_categorical\n",
    "    #reshape y\n",
    "    y_train = y_train.reshape((-1,1))\n",
    "    y_test = y_test.reshape((-1,1))\n",
    "    \n",
    "    #cast to np.float32\n",
    "    x_train = x_train.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    x_test = x_test.astype(np.float32)\n",
    "    y_test = y_test.astype(np.float32)\n",
    "    \n",
    "    # Compute the categorical classes (doenst requested when binary classification)\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
    "     \n",
    "    # reshape x \n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1]* x_train.shape[2], order='F')\n",
    "    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1]* x_test.shape[2], order='F') \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedd159e",
   "metadata": {},
   "source": [
    "## Functions for FeedForwardNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a60dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):    \n",
    "    return 1.0/(1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cba43df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x):    \n",
    "#forward pass - preactivation and activation    \n",
    "    x1, x2 = x    \n",
    "    a1 = w1*x1 + w2*x2 + b1    \n",
    "    h1 = sigmoid(a1)    \n",
    "    a2 = w3*x1 + w4*x2 + b2    \n",
    "    h2 = sigmoid(a2)   \n",
    "    a3 = w5*self.h1 + self.w6*self.h2 + self.b3    \n",
    "    h3 = self.sigmoid(self.a3)    \n",
    "    return self.h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8c59b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095811de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee2bd70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a0399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2532c555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c7803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
